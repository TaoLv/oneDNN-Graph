<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>oneDNN Graph: Getting Started with Programming Model</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div class="mobile-nav"><i id="nav-btn"></i><a href="index.html">oneAPI Deep Neural Network Library Graph API (oneDNN Graph)</a></div>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">
     <a href="index.html">
      <div id="full-name">oneAPI Deep Neural Network Library Graph API (oneDNN Graph)</div>
    </a>
   </div>
   <div id="projectbrief">Performance Library for DNN Graph Optimization</div>
   <div id="projectnumber">0.4.3</div>
  <div>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.svg"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('dev_guide_cpu_programming.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Getting Started with Programming Model </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_doc_programming_model_cpu_programming"></a> </p>
<h1><a class="anchor" id="autotoc_md19"></a>
Description</h1>
<p>Previously, oneDNN Graph assumes that users have a graph representation and a graph executor which drives the overall execution. That means, oneDNN Graph supports constructing a graph by users, but has limited support for users' program to directly compile and execute the returned partitions.</p>
<p>Now, oneDNN Graph features the support of minimum programming model. Users can easily construct a self-defined graph and generate the corresponding partitions. After that, users can compile and execute those partitions.</p>
<p>Here, an example will be provided to show the programming model. The full example code can be found at <a href="../../tests/demo/src/cpu_programming.cpp">cpu_programming.cpp</a>.</p>
<h1><a class="anchor" id="autotoc_md20"></a>
cpu_programming_tutorial() function</h1>
<h2><a class="anchor" id="autotoc_md21"></a>
Create tensor mapping</h2>
<p>In order to provide a running context, a new class named <a href="../../tests/demo/include/common/execution_context.hpp#L53">tensor_map</a> is being introduced in this example. A tensor map will be responsible for holding all the tensors that will be used in the users' program. it contains the mapping from an unique logical tensor id to the corresponding tensor.</p>
<ul>
<li><code>std::unordered_map&lt;size_t, tensor&gt; data_</code></li>
</ul>
<p>Before building graph, users should create such a tensor map like below:</p>
<div class="fragment"><div class="line">tensor_map tm;</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md22"></a>
Build graph and get partitions</h2>
<p>First of all, a <code>graph</code> is needed since it will help users to construct the self-defined graph.</p>
<div class="fragment"><div class="line">graph g(engine_kind);</div>
</div><!-- fragment --><p>In this example, the below graph will be used. It contains <code>Convolution</code>, <code>Wildcard</code>, <code>Add</code>, <code>ReLU</code> and <code>End</code> ops.</p>
<div class="fragment"><div class="line"> Convolution Wildcard</div>
<div class="line">      |         |</div>
<div class="line">   tensor1   tensor2</div>
<div class="line">  /      \     /</div>
<div class="line">End        Add</div>
<div class="line">            |</div>
<div class="line">         tensor3</div>
<div class="line">            |</div>
<div class="line">           ReLU</div>
<div class="line">            |</div>
<div class="line">         tensor4</div>
<div class="line">            |</div>
<div class="line">           End</div>
</div><!-- fragment --><p>In oneDNN Graph, the id of <a href="../../include/oneapi/dnnl/dnnl_graph.hpp#L290">dnnl::graph::logical_tensor</a> is used to express the connection relationship between different ops. So for the first <code>Convolution</code> op, users can construct all input and output logical tensors like below.</p>
<div class="fragment"><div class="line">logical_tensor conv_data_lt {id_mgr[<span class="stringliteral">&quot;conv_data&quot;</span>], data_type::f32, input_dims, layout_type::strided};</div>
<div class="line">logical_tensor conv_weight_lt {id_mgr[<span class="stringliteral">&quot;conv_weight&quot;</span>], data_type::f32, weight_dims, layout_type::strided};</div>
<div class="line">logical_tensor conv_bias_lt {id_mgr[<span class="stringliteral">&quot;conv_bias&quot;</span>], data_type::f32, bias_dims, layout_type::strided};</div>
<div class="line">logical_tensor conv_dst_lt {id_mgr[<span class="stringliteral">&quot;dst_dims&quot;</span>], data_type::f32, dst_dims, layout_type::strided};</div>
</div><!-- fragment --><p>Here <a href="../../tests/demo/include/common/utils.hpp#L135"><code>id_mgr</code></a> is a utility class to generate unique id according to the given name. It requires the 1:1 mapping between id and the given name.</p>
<p><b>Note</b>: These examples create logical tensors with complete shape information and use them in the partition compilation. Currently, oneDNN Graph also supports the output logical tensors with incomplete shape information (containing -1). oneDNN Graph implementation will calculate the output shapes according to given input shapes and schema of the OP. After compilation finished, users can query the compiled partition for the output logical tensors and get the shapes.</p>
<p>Next step is to create a <code>Convolution</code> op with the above inputs and outputs.</p>
<div class="fragment"><div class="line">op conv {0, op::kind::Convolution, {conv_data_lt, conv_weight_lt, conv_bias_lt}, {conv_dst_lt}, <span class="stringliteral">&quot;conv_0&quot;</span>};</div>
<div class="line">conv.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;strides&quot;</span>, {1, 1});</div>
<div class="line">conv.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;pads_begin&quot;</span>, {0, 0});</div>
<div class="line">conv.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;pads_end&quot;</span>, {0, 0});</div>
<div class="line">conv.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;dilations&quot;</span>, {1, 1});</div>
<div class="line">conv.set_attr&lt;int64_t&gt;(<span class="stringliteral">&quot;groups&quot;</span>, 1);</div>
<div class="line">conv.set_attr&lt;std::string&gt;(<span class="stringliteral">&quot;data_format&quot;</span>, <span class="stringliteral">&quot;NCX&quot;</span>);</div>
<div class="line">conv.set_attr&lt;std::string&gt;(<span class="stringliteral">&quot;filter_format&quot;</span>, <span class="stringliteral">&quot;OIX&quot;</span>);</div>
</div><!-- fragment --><p>The similar way can be used to construct other logical tensors and ops.</p>
<div class="fragment"><div class="line">logical_tensor add_input_2_lt {id_mgr[<span class="stringliteral">&quot;add_input_2&quot;</span>], data_type::f32, add_input_2_dims, layout_type::strided};</div>
<div class="line">logical_tensor add_dst_lt {id_mgr[<span class="stringliteral">&quot;add_dst&quot;</span>], data_type::f32, dst_dims, layout_type::strided};</div>
<div class="line"> </div>
<div class="line">op add {1, op::kind::Add, {conv_dst_lt, add_input_2_lt}, {add_dst_lt}, <span class="stringliteral">&quot;add_0&quot;</span>};</div>
<div class="line"> </div>
<div class="line">logical_tensor relu_dst_lt {id_mgr[<span class="stringliteral">&quot;relu_dst&quot;</span>], data_type::f32, dst_dims, layout_type::strided};</div>
<div class="line"> </div>
<div class="line">op relu {2, op::kind::ReLU, {add_dst_lt}, {relu_dst_lt}, <span class="stringliteral">&quot;relu_0&quot;</span>};</div>
<div class="line"> </div>
<div class="line">op wildcard {3, op::kind::Wildcard, {}, {add_input_2_lt}, <span class="stringliteral">&quot;wildcard_0&quot;</span>};</div>
</div><!-- fragment --><p>Users are also able to specify the output(s) of the graph through <code>End</code> op. Below code is used to specify the outputs of <code>Convolution</code> and <code>ReLU</code> ops, which are also the outputs of the whole graph.</p>
<div class="fragment"><div class="line">op end_0 {4, op::kind::End, {conv_dst_lt}, {}, <span class="stringliteral">&quot;end_0&quot;</span>};</div>
<div class="line">op end_1 {5, op::kind::End, {relu_dst_lt}, {}, <span class="stringliteral">&quot;end_1&quot;</span>};</div>
</div><!-- fragment --><p>After all those ops are created, users can add these ops into the graph.</p>
<div class="fragment"><div class="line">g.add_op(conv);</div>
<div class="line">g.add_op(add);</div>
<div class="line">g.add_op(relu);</div>
<div class="line">g.add_op(wildcard);</div>
<div class="line">g.add_op(end_0);</div>
<div class="line">g.add_op(end_1);</div>
</div><!-- fragment --><p>Then by calling <a href="../../include/oneapi/dnnl/dnnl_graph.hpp#L1284"><code>get_partitions()</code></a>, users can get several partitions in topological order.</p>
<div class="fragment"><div class="line">std::vector&lt;partition&gt; partitions = g.get_partitions();</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md23"></a>
Compile partitions and execute</h2>
<p>In the real workload, users need to provide the device information to compile a partition. Typically, a <a href="../../include/oneapi/dnnl/dnnl_graph.hpp#L97">dnnl::graph::engine</a> should be created with an engine kind and a device id. The engine kind should be the same as the one used to create the graph.</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> device_id = 0;</div>
<div class="line">engine e {engine_kind, device_id};</div>
</div><!-- fragment --><p>In oneDNN Graph, a <a href="../../include/oneapi/dnnl/dnnl_graph.hpp#L239">dnnl::graph::stream</a> is the logical abstraction for execution units. It is created on top of oneDNN Graph engine.</p>
<div class="fragment"><div class="line">stream s {e};</div>
</div><!-- fragment --><p>Currently, a partition also has a flag to indicate the supporting status. If the flag is True, that partition is supported by oneDNN Graph backend. Otherwise, that partition is not supported by oneDNN Graph backend and users need to handle the computation by themselves.</p>
<ul>
<li><a href="../../include/oneapi/dnnl/dnnl_graph.hpp#L1149"><code>is_supported()</code></a></li>
</ul>
<div class="fragment"><div class="line"><span class="comment">// create the vector to store all compiled partitions</span></div>
<div class="line">std::vector&lt;compiled_partition&gt; c_partitions(partitions.size());</div>
<div class="line"><span class="comment">// compilation-execution loop</span></div>
<div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; partitions.size(); ++i) {</div>
<div class="line">    <span class="keywordflow">if</span> (partitions[i].is_supported()) {</div>
<div class="line">        <span class="comment">// 1. get inputs and outputs from a partition</span></div>
<div class="line">        <span class="comment">// 2. compile the partition to generate a compile partition</span></div>
<div class="line">        <span class="comment">// 3. construct tensors with logical tensors and allocated memory buffer</span></div>
<div class="line">        <span class="comment">// 4. execute the compiled partition with the stream</span></div>
<div class="line">    } <span class="keywordflow">else</span> {</div>
<div class="line">        <span class="comment">// users need to write code to compute this partition</span></div>
<div class="line">    }</div>
<div class="line">}</div>
</div><!-- fragment --><p>At the compilation stage, users need to provide input and output logical tensors for oneDNN Graph compilation API. The below APIs will be used for this purpose.</p>
<ul>
<li><code>partition.get_in_ports()</code>: return the list of input logical tensors from the partition</li>
<li><code>partition.get_out_ports()</code>: return the list of output logical tensors from the partition</li>
</ul>
<div class="fragment"><div class="line">std::vector&lt;logical_tensor&gt; inputs = partitions[i].get_in_ports();</div>
<div class="line">std::vector&lt;logical_tensor&gt; outputs = partitions[i].get_out_ports();</div>
</div><!-- fragment --><p>Then, users can compile the partition according to the given inputs/outputs.</p>
<div class="fragment"><div class="line">c_partitions[i] = partitions[i].compile(inputs, outputs, e);</div>
</div><!-- fragment --><p>Before executing, users need to construct input and output tensors with memory buffer. So, a helper function is provided in the example like below:</p>
<ul>
<li><code>construct_and_initialize_tensors()</code>: construct and initialize tensors according to the given logical tensors.</li>
</ul>
<div class="fragment"><div class="line">std::vector&lt;tensor&gt; input_ts = construct_and_initialize_tensors(inputs, c_partitions[i], tm, e, 1);</div>
<div class="line">std::vector&lt;tensor&gt; output_ts = construct_and_initialize_tensors(outputs, c_partitions[i], tm, e, 0);</div>
</div><!-- fragment --><p>Finally, users can execute the compiled partition with input and output tensors.</p>
<div class="fragment"><div class="line">c_partitions[i].execute(s, input_ts, output_ts);</div>
</div><!-- fragment --><p>After finishing executing all compiled partitions, users can get the final results of the graph.</p>
<h1><a class="anchor" id="autotoc_md24"></a>
Single Operator Partition</h1>
<p>In order to simplify the support of framework imperative execution mode, oneDNN Graph API supports single op partition. As the name suggests, it's a partition which only contains one operator. There is no need to use graph for imperative execution mode. The demo code is like below. The full example code can be found at <a href="../../examples/cpp/cpu_single_op_partition_f32.cpp">cpu_single_op_partition_f32.cpp</a>.</p>
<div class="fragment"><div class="line"><span class="comment">// define input and output logical tensor</span></div>
<div class="line">logical_tensor lt0 {0, data_type::f32, {1, 3, 256, 256}, layout_type::strided}; <span class="comment">// 1</span></div>
<div class="line">logical_tensor lt1 {1, data_type::f32, {32, 3, 3, 3}, layout_type::strided};    <span class="comment">// 2</span></div>
<div class="line">logical_tensor lt2 {2, data_type::f32, {1, 32, 252, 252}, layout_type::strided};<span class="comment">// 3</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// step 1: create the op</span></div>
<div class="line">op conv {0, kind::Convolution, {lt0, lt1}, {lt2}, <span class="stringliteral">&quot;convolution&quot;</span>};               <span class="comment">// 4</span></div>
<div class="line"><span class="comment">// set many attributes</span></div>
<div class="line">conv.set_attr(<span class="stringliteral">&quot;pads_begin&quot;</span>, {0,0});                                             <span class="comment">// 5</span></div>
<div class="line">conv.set_attr(<span class="stringliteral">&quot;pads_end&quot;</span>, {0,0});                                               <span class="comment">// 6</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// step 2: create partition with the given operator</span></div>
<div class="line">partition part {conv, engine_kind};                                             <span class="comment">// 7</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// step 3: compile the partition</span></div>
<div class="line">engine eng {engine_kind, 0};                                                    <span class="comment">// 8</span></div>
<div class="line">compiled_partition cpart = part.compile({lt0, lt1}, {lt2}, eng);                <span class="comment">// 9</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// step 4: execute the compiled partition</span></div>
<div class="line">tensor data {lt0, eng, buf0};                                                   <span class="comment">// 10</span></div>
<div class="line">tensor weight {lt1, eng, buf1};                                                 <span class="comment">// 11</span></div>
<div class="line">tensor output {lt2, eng, buf2};                                                 <span class="comment">// 12</span></div>
<div class="line">cp.execute(stream, {data, weight}, {output});                                   <span class="comment">// 13</span></div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md25"></a>
Supporting Fall-back Mechanism for Users</h1>
<p>Sometimes framework users may want to fall back to default implementation when oneDNN Graph fails to compile or execute. At this time users need to reorder oneDNN Graph opaque tensors to public tensors and then feed into framework default implementation kernel. oneDNN Graph API supports <code>reorder</code> operator for this case. With the help of feature of single operator partition, users can easily convert tensors to public layout.</p>
<h1><a class="anchor" id="autotoc_md26"></a>
Weight Prepacking</h1>
<p>For inference mode, weights are usually constant during iterations. In order to improve inference performance, users may want to convert the weight from public layout to opaque layout and use this converted weight every iteration. This will significantly reduce the overhead of converting weight every iteration. With the combination of single operator partition and <code>reorder</code> operation, users are able to implement this optimization on their side. The weight's opaque layout can be only queried out from compiled partition, which requires the tensor shapes must be known at the compilation time.</p>
<h1><a class="anchor" id="autotoc_md27"></a>
Additional Ease-of-Use Features</h1>
<ul>
<li>Users are not required to set the tensors' format<ul>
<li>oneDNN Graph API removes the format representation from logical tensor, so users don't need to indicate whether the tensor is NHWC tensor or NCHW tensor. Instead, the logical tensor used in oneDNN Graph API only needs to describe the layout using dims and strides. The semantics of axes for tensors are specified by the operator's attributes. For example, the Convolution operator has <code>data_format</code> and <code>filter_format</code>.</li>
</ul>
</li>
</ul>
<div class="fragment"><div class="line"><span class="comment">// for example, tensorflow shapes</span></div>
<div class="line">dims_t ish = {1, 227, 227, 3};</div>
<div class="line">dims_t wsh = {11, 11, 3, 96};</div>
<div class="line">dims_t osh = {1, 55, 55, 96};</div>
<div class="line"><span class="comment">// create input/output logical tensors.</span></div>
<div class="line">logical_tensor conv_src {0, dt::f32, ish, layout_type::strided};</div>
<div class="line">logical_tensor conv_wei {1, dt::f32, wsh, layout_type::strided};</div>
<div class="line">logical_tensor conv_dst {2, dt::f32, osh, layout_type::strided};</div>
<div class="line"><span class="comment">// create convolution op with inputs and outputs</span></div>
<div class="line">op conv {0, kind::Convolution, {conv_src, conv_wei}, {conv_dst}, “conv0”};</div>
<div class="line"><span class="comment">// set attributes to conv op</span></div>
<div class="line">conv.set_attr(<span class="stringliteral">&quot;data_format&quot;</span>, std::string(<span class="stringliteral">&quot;NXC&quot;</span>)); <span class="comment">// input, nhwc</span></div>
<div class="line">conv.set_attr(<span class="stringliteral">&quot;filter_format&quot;</span>, std::string(<span class="stringliteral">&quot;XIO&quot;</span>)); <span class="comment">// weight, hwio</span></div>
<div class="line"><span class="comment">// add op to graph.</span></div>
<div class="line">graph.add_op(conv);</div>
</div><!-- fragment --><ul>
<li>Users are not required to "query and convert" to the opaque layout for input tensors at execution time<ul>
<li>oneDNN Graph API allows that the output tensor of one partition is directly passed to another partition, even though this output tensor has opaque layout.</li>
</ul>
</li>
</ul>
<div class="fragment"><div class="line"><span class="comment">// create logical tensors, previous layer also runs oneDNN Graph partition</span></div>
<div class="line">logical_tensor conv_src {0, dt::f32, {1, 3, 227, 227}, layout_type::opaque};</div>
<div class="line">logical_tensor conv_wei {1, dt::f32, {96, 3, 11, 11}, layout_type::strided};</div>
<div class="line">logical_tensor conv_dst {2, dt::f32, {-1, -1, -1, -1}, layout_type::any};</div>
<div class="line"><span class="comment">// create convolution op with inputs and outputs</span></div>
<div class="line">op conv {0, kind::Convolution, {conv_src, conv_wei}, {conv_dst}, “conv0”};</div>
<div class="line"><span class="comment">// add op to graph.</span></div>
<div class="line">graph.add_op(conv);</div>
<div class="line"><span class="comment">// get partitions with debug policy.</span></div>
<div class="line">std::vector&lt;partition&gt; partitions = graph.get_partitions();</div>
<div class="line"><span class="comment">// compile the first partition and set blocked format to output tensor</span></div>
<div class="line">compiled_partition cp = partitions[0].compile({conv_src, conv_wei}, {conv_dst}, eng);</div>
<div class="line"><span class="comment">// execute the compiled partition with input/output tensors</span></div>
<div class="line">tensor src_tensor = tensor_from_last_layer; <span class="comment">// with opaque layout id</span></div>
<div class="line">tensor wei_tensor = tensor(conv_wei, eng, buf_wei); <span class="comment">// strided weight</span></div>
<div class="line">tensor dst = tensor(cp.query_logical_tensor(2), eng, buf_dst);</div>
<div class="line">cp.execute(stream, {src_tensor, wei_tensor}, {dst_tensor});</div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="footer">
    <script>
        $('#top').prependTo($('#side-nav'));
    </script>
    <div class="footer-wrapper">
        <hr>
        <ul class="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>