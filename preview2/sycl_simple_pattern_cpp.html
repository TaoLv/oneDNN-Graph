<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>oneDNN Graph: Getting started on both CPU and GPU with SYCL extensions API</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div class="mobile-nav"><i id="nav-btn"></i><a href="index.html">oneAPI Deep Neural Network Library Graph API (oneDNN Graph)</a></div>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">
     <a href="index.html">
      <div id="full-name">oneAPI Deep Neural Network Library Graph API (oneDNN Graph)</div>
    </a>
   </div>
   <div id="projectbrief">Performance Library for DNN Graph Optimization</div>
   <div id="projectnumber">0.2.0</div>
  <div>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.svg"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('sycl_simple_pattern_cpp.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Getting started on both CPU and GPU with SYCL extensions API </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This is an example to demonstrate how to build a simple graph and run on SYCL device.</p>
<blockquote class="doxtable">
<p>Example code: <a class="el" href="sycl_simple_pattern_8cpp-example.html">sycl_simple_pattern.cpp</a> </p>
</blockquote>
<p>Some key take-aways included in this example:</p>
<ul>
<li>how to build a graph and get several partitions</li>
<li>how to create engine, allocator and stream</li>
<li>how to compile a partition</li>
<li>how to execute a compiled partition with input tensors on a specific stream</li>
</ul>
<h1><a class="anchor" id="sycl_simple_pattern_cpp_headers"></a>
Public headers</h1>
<p>To start using oneDNN graph, we must include the <a class="el" href="dnnl__graph_8hpp_source.html">dnnl_graph.hpp</a> header file in the application. If you also want to run with SYCL device, you need include <a class="el" href="dnnl__graph__sycl_8hpp_source.html">dnnl_graph_sycl.hpp</a> header file as well. All the C++ APIs reside in namespace <code>dnnl::graph</code>.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;oneapi/dnnl/dnnl_graph.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;oneapi/dnnl/dnnl_graph_sycl.hpp&quot;</span></div>
<div class="line"><span class="keyword">using namespace </span>dnnl::graph;</div>
<div class="line"><span class="keyword">using namespace </span>cl::sycl;</div>
</div><!-- fragment --> <h1><a class="anchor" id="sycl_simple_pattern_cpp_tutorial"></a>
sycl_simple_pattern_tutorial() function</h1>
<h2><a class="anchor" id="sycl_simple_pattern_cpp_get_partition"></a>
Build graph and get partitions</h2>
<p>In this section, we are trying to build a graph containing the pattern like <code>conv0-&gt;relu0-&gt;conv1-&gt;relu1</code>. After that, we can get all of partitions which are determined by backend.</p>
<p>To create a graph, <a class="el" href="classdnnl_1_1graph_1_1engine.html#a425168b38184f5d22b9eebfb3193b40e" title="engine kind">dnnl::graph::engine::kind</a> is needed because the returned partitions maybe vary on different devices. </p><div class="fragment"><div class="line">    graph g(engine_kind);</div>
</div><!-- fragment --><p> To build a graph, the connection relationship of different ops must be known. In oneDNN graph, <a class="el" href="classdnnl_1_1graph_1_1logical__tensor.html" title="Logical tensor object.">dnnl::graph::logical_tensor</a> is used to express such relationship. So, next step is to create logical tensors for these ops including inputs and outputs.</p>
<p>Create input/output <a class="el" href="classdnnl_1_1graph_1_1logical__tensor.html" title="Logical tensor object.">dnnl::graph::logical_tensor</a> for first <code>Convolution</code> op. </p><div class="fragment"><div class="line">    logical_tensor conv0_src_desc {logical_id[0], logical_tensor::data_type::f32, input_dims, logical_tensor::layout_type::strided};</div>
<div class="line">    logical_tensor conv0_weight_desc {logical_id[1], logical_tensor::data_type::f32, weight_dims, logical_tensor::layout_type::strided};</div>
<div class="line">    logical_tensor conv0_dst_desc {logical_id[2], logical_tensor::data_type::f32, dst_dims, logical_tensor::layout_type::strided};</div>
</div><!-- fragment --><p>Create first <code>Convolution</code> op (<a class="el" href="classdnnl_1_1graph_1_1op.html" title="A op contains kind, attribute, and the input and output logical tensor(s).">dnnl::graph::op</a>) and attaches attributes to it, such as <code>strides</code>, <code>pads_begin</code>, <code>pads_end</code>, <code>data_format</code>, etc. </p><div class="fragment"><div class="line">    op conv0(0, op::kind::Convolution, {conv0_src_desc, conv0_weight_desc}, {conv0_dst_desc}, <span class="stringliteral">&quot;conv0&quot;</span>);</div>
<div class="line">    conv0.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;strides&quot;</span>, {4, 4});</div>
<div class="line">    conv0.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;pads_begin&quot;</span>, {0, 0});</div>
<div class="line">    conv0.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;pads_end&quot;</span>, {0, 0});</div>
<div class="line">    conv0.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;dilations&quot;</span>, {1, 1});</div>
<div class="line">    conv0.set_attr&lt;int64_t&gt;(<span class="stringliteral">&quot;groups&quot;</span>, 1);</div>
<div class="line">    conv0.set_attr&lt;std::string&gt;(<span class="stringliteral">&quot;data_format&quot;</span>, <span class="stringliteral">&quot;NCX&quot;</span>);</div>
<div class="line">    conv0.set_attr&lt;std::string&gt;(<span class="stringliteral">&quot;filter_format&quot;</span>, <span class="stringliteral">&quot;OIX&quot;</span>);</div>
</div><!-- fragment --><p>Create input/output logical tensors for first <code>BiasAdd</code> op. </p><div class="fragment"><div class="line">    logical_tensor conv0_bias_desc {logical_id[3], logical_tensor::data_type::f32, bias_dims, logical_tensor::layout_type::strided};</div>
<div class="line">    logical_tensor conv0_bias_add_dst_desc {logical_id[4], logical_tensor::data_type::f32, dst_dims, logical_tensor::layout_type::strided};</div>
</div><!-- fragment --><p>Create first <code>BiasAdd</code> op. </p><div class="fragment"><div class="line">    op conv0_bias_add(1, op::kind::BiasAdd, {conv0_dst_desc, conv0_bias_desc}, {conv0_bias_add_dst_desc}, <span class="stringliteral">&quot;conv0_bias_add&quot;</span>);</div>
</div><!-- fragment --><p>Create output logical tensors for first <code>Relu</code> op. </p><div class="fragment"><div class="line">    logical_tensor relu0_dst_desc {logical_id[5], logical_tensor::data_type::f32, dst_dims, logical_tensor::layout_type::strided};</div>
</div><!-- fragment --><p>Create first <code>Relu</code> op. </p><div class="fragment"><div class="line">    op relu0(2, op::kind::ReLU, {conv0_bias_add_dst_desc}, {relu0_dst_desc}, <span class="stringliteral">&quot;relu0&quot;</span>);</div>
</div><!-- fragment --><p>Create input/output logical tensors for second <code>Convolution</code> op. </p><div class="fragment"><div class="line">    logical_tensor conv1_weight_desc {logical_id[6], logical_tensor::data_type::f32, weight1_dims, logical_tensor::layout_type::strided};</div>
<div class="line">    logical_tensor conv1_dst_desc {logical_id[7], logical_tensor::data_type::f32, dst_dims, logical_tensor::layout_type::strided};</div>
</div><!-- fragment --><p>Create second <code>Convolution</code> op and also attaches required attributes to it. </p><div class="fragment"><div class="line">    op conv1(3, op::kind::Convolution, {relu0_dst_desc, conv1_weight_desc}, {conv1_dst_desc}, <span class="stringliteral">&quot;conv1&quot;</span>);</div>
<div class="line">    conv1.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;strides&quot;</span>, {1, 1});</div>
<div class="line">    conv1.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;pads_begin&quot;</span>, {0, 0});</div>
<div class="line">    conv1.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;pads_end&quot;</span>, {0, 0});</div>
<div class="line">    conv1.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(<span class="stringliteral">&quot;dilations&quot;</span>, {1, 1});</div>
<div class="line">    conv1.set_attr&lt;int64_t&gt;(<span class="stringliteral">&quot;groups&quot;</span>, 1);</div>
<div class="line">    conv1.set_attr&lt;std::string&gt;(<span class="stringliteral">&quot;data_format&quot;</span>, <span class="stringliteral">&quot;NCX&quot;</span>);</div>
<div class="line">    conv1.set_attr&lt;std::string&gt;(<span class="stringliteral">&quot;filter_format&quot;</span>, <span class="stringliteral">&quot;OIX&quot;</span>);</div>
</div><!-- fragment --><p>Create input/output logical tensors for second <code>BiasAdd</code> op. </p><div class="fragment"><div class="line">    logical_tensor conv1_bias_desc {logical_id[8], logical_tensor::data_type::f32, bias1_dims, logical_tensor::layout_type::strided};</div>
<div class="line">    logical_tensor conv1_bias_add_dst_desc {logical_id[9], logical_tensor::data_type::f32, dst_dims, logical_tensor::layout_type::strided};</div>
</div><!-- fragment --><p>Create second <code>BiasAdd</code> op. </p><div class="fragment"><div class="line">    op conv1_bias_add(4, op::kind::BiasAdd, {conv1_dst_desc, conv1_bias_desc}, {conv1_bias_add_dst_desc}, <span class="stringliteral">&quot;conv1_bias_add&quot;</span>);</div>
</div><!-- fragment --><p>Create output logical tensors for second <code>Relu</code> op. </p><div class="fragment"><div class="line">    logical_tensor relu1_dst_desc {logical_id[10], logical_tensor::data_type::f32, dst_dims, logical_tensor::layout_type::strided};</div>
</div><!-- fragment --><p>Create second <code>Relu</code> op. </p><div class="fragment"><div class="line">    op relu1(5, op::kind::ReLU, {conv1_bias_add_dst_desc}, {relu1_dst_desc}, <span class="stringliteral">&quot;relu1&quot;</span>);</div>
</div><!-- fragment --><p>Finally, those created ops will be added into the graph. The graph inside will maintain a list to store all these ops. </p><div class="fragment"><div class="line">    g.add_op(conv0);</div>
<div class="line">    g.add_op(relu0);</div>
<div class="line">    g.add_op(conv1);</div>
<div class="line">    g.add_op(relu1);</div>
<div class="line">    g.add_op(conv0_bias_add);</div>
<div class="line">    g.add_op(conv1_bias_add);</div>
</div><!-- fragment --><p>After finished above operations, we can get partitions by calling <a class="el" href="classdnnl_1_1graph_1_1graph.html#a8f549cae111ab22b6c656e7c66a35b03" title="Get filtered partitions.">dnnl::graph::graph::get_partitions()</a>. Here we can slao specify the <a class="el" href="classdnnl_1_1graph_1_1partition.html#a439c0490ea8ea85f2a12ec7b320a9a3c" title="Policy specifications for partitioning.">dnnl::graph::partition::policy</a> to get different partitions.</p>
<p>In this example, the graph will be filtered into two partitions: <code>conv0+relu0</code> and <code>conv1+relu1</code>.</p>
<dl class="section note"><dt>Note</dt><dd>Setting <code>DNNL_GRAPH_DUMP=1</code> can save internal graphs into dot files before/after graph fusion.</dd></dl>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> partitions = g.get_partitions();</div>
</div><!-- fragment --><p>mark the output logical tensors of partition as ANY layout enabled</p>
<h2><a class="anchor" id="sycl_simple_pattern_cpp_compile"></a>
Compile partition</h2>
<p>In the real case, we assume that framework can provide device info at this stage. But in this example, we just use a self-defined device to simulate the real behavior.</p>
<p>Create a <a class="el" href="classdnnl_1_1graph_1_1allocator.html" title="Allocator.">dnnl::graph::allocator</a> with two user-defined <a class="el" href="group__dnnl__graph__api__allocator.html#gac3c4f5c1bc48528891f0119d12e5160a" title="Allocation call-back function interface for SYCL device.">dnnl_graph_sycl_allocate_f</a> and <a class="el" href="group__dnnl__graph__api__allocator.html#gae42aaa4841665c015d59625fab54d108" title="Deallocation call-back function interface for SYCL device.">dnnl_graph_sycl_deallocate_f</a> call-back functions. </p><div class="fragment"><div class="line">    allocator alloc = sycl_interop::make_allocator(sycl_malloc_wrapper, sycl_free_wrapper);</div>
</div><!-- fragment --><p> Define SYCL queue (code outside of oneDNN graph) </p><div class="fragment"><div class="line">    sycl::queue q = (engine_kind == engine::kind::gpu) ? sycl::queue(gpu_selector {}, sycl::property::queue::in_order {}) : sycl::queue(cpu_selector {}, sycl::property::queue::in_order {});</div>
</div><!-- fragment --><p>Create a <a class="el" href="classdnnl_1_1graph_1_1engine.html" title="An engine contains device kind and a device_id or device_handle.">dnnl::graph::engine</a> based on SYCL device and context. Also, set a user-defined <a class="el" href="classdnnl_1_1graph_1_1allocator.html" title="Allocator.">dnnl::graph::allocator</a> to this engine.</p>
<div class="fragment"><div class="line">    engine eng = sycl_interop::make_engine(q.get_device(), q.get_context());</div>
<div class="line">    eng.set_allocator(alloc);</div>
</div><!-- fragment --><p>Create a stream on the engine associated with a sycl queue. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> strm = sycl_interop::make_stream(eng, q);</div>
</div><!-- fragment --><p>replace input logical tensor with the queried one</p>
<p>update output logical tensors with ANY layout</p>
<p>compile to generate compiled partition </p><div class="fragment"><div class="line">            c_partitions[i] = partitions[i].compile(inputs, outputs, eng);</div>
</div><!-- fragment --><p>execute the compiled partition </p><div class="fragment"><div class="line">            sycl_interop::execute(c_partitions[i], strm, input_ts, output_ts);</div>
</div><!-- fragment --><p>Users need to write implementation code by themselves.</p>
<p>Check correctness of the output results. </p><div class="fragment"><div class="line">    <span class="keywordtype">float</span> expected_result</div>
<div class="line">            = (1 * 11 * 11 * 3 + <span class="comment">/* conv0 bias */</span> 1.0f) * (1 * 1 * 96)</div>
<div class="line">            + <span class="comment">/* conv1 bias */</span> 1.0f;</div>
<div class="line">    <span class="keywordtype">float</span> *actual_output_ptr = tm.get(relu1_dst_desc.get_id()).get_data_handle&lt;float&gt;();</div>
<div class="line">    <span class="keyword">auto</span> output_dims = relu1_dst_desc.get_dims();</div>
<div class="line">    <span class="keyword">auto</span> num_elem = product(output_dims);</div>
<div class="line">    std::vector&lt;float&gt; expected_output(num_elem, expected_result);</div>
<div class="line">    compare_data(expected_output.data(), actual_output_ptr, num_elem);</div>
</div><!-- fragment --></div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="footer">
    <script>
        $('#top').prependTo($('#side-nav'));
    </script>
    <div class="footer-wrapper">
        <hr>
        <ul class="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>