<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>oneDNN Graph: Getting started on both CPU and GPU with SYCL extensions API</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div class="mobile-nav"><i id="nav-btn"></i><a href="index.html">oneAPI Deep Neural Network Library Graph API (oneDNN Graph)</a></div>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">
     <a href="index.html">
      <div id="full-name">oneAPI Deep Neural Network Library Graph API (oneDNN Graph)</div>
    </a>
   </div>
   <div id="projectbrief">Performance Library for DNN Graph Optimization</div>
   <div id="projectnumber">0.8.1</div>
  <div>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.svg"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('sycl_get_started_cpp.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Getting started on both CPU and GPU with SYCL extensions API </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This is an example to demonstrate how to build a simple graph and run on SYCL device.</p>
<blockquote class="doxtable">
<p>Example code: <a class="el" href="sycl_get_started_8cpp-example.html">sycl_get_started.cpp</a> </p>
</blockquote>
<p>Some key take-aways included in this example:</p>
<ul>
<li>how to build a graph and get several partitions</li>
<li>how to create engine, allocator and stream</li>
<li>how to compile a partition</li>
<li>how to execute a compiled partition</li>
</ul>
<p>Some assumptions in this example:</p>
<ul>
<li>Only workflow is demonstrated without checking correctness</li>
<li>Unsupported partitions should be handled by users themselves</li>
</ul>
<h1><a class="anchor" id="sycl_get_started_cpp_headers"></a>
Public headers</h1>
<p>To start using oneDNN graph, we must include the <a class="el" href="dnnl__graph_8hpp_source.html">dnnl_graph.hpp</a> header file into the application. If you also want to run with SYCL device, you need include <a class="el" href="dnnl__graph__sycl_8hpp_source.html">dnnl_graph_sycl.hpp</a> header as well. All the C++ APIs reside in namespace <code><a class="el" href="namespacednnl_1_1graph.html" title="oneDNN Graph API namespace">dnnl::graph</a></code>.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;oneapi/dnnl/dnnl_graph.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;oneapi/dnnl/dnnl_graph_sycl.hpp&quot;</span></div>
<div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacednnl_1_1graph.html">dnnl::graph</a>;</div>
<div class="line"><span class="keyword">using namespace </span>sycl;</div>
<div class="ttc" id="anamespacednnl_1_1graph_html"><div class="ttname"><a href="namespacednnl_1_1graph.html">dnnl::graph</a></div><div class="ttdoc">oneDNN Graph API namespace</div><div class="ttdef"><b>Definition:</b> dnnl_graph.hpp:38</div></div>
</div><!-- fragment --> <h1><a class="anchor" id="sycl_get_started_cpp_tutorial"></a>
sycl_get_started_tutorial() function</h1>
<h2><a class="anchor" id="sycl_get_started_cpp_get_partition"></a>
Build graph and get partitions</h2>
<p>In this section, we are trying to build a graph containing the pattern like <code>conv0-&gt;relu0-&gt;conv1-&gt;relu1</code>. After that, we can get all of partitions which are determined by backend.</p>
<p>To create a graph, <a class="el" href="classdnnl_1_1graph_1_1engine.html#a425168b38184f5d22b9eebfb3193b40e" title="Kinds of engine.">dnnl::graph::engine::kind</a> is needed because the returned partitions maybe vary on different devices. </p><div class="fragment"><div class="line">    graph g(ekind);</div>
</div><!-- fragment --><p> To build a graph, the connection relationship of different ops must be known. In oneDNN graph, <a class="el" href="classdnnl_1_1graph_1_1logical__tensor.html" title="Logical tensor object.">dnnl::graph::logical_tensor</a> is used to express such relationship. So, next step is to create logical tensors for these ops including inputs and outputs.</p>
<dl class="section note"><dt>Note</dt><dd>It's not necessary to provide concrete shape/layout information at graph partitioning stage. Users can provide these information till compilation stage.</dd></dl>
<p>Create input/output <a class="el" href="classdnnl_1_1graph_1_1logical__tensor.html" title="Logical tensor object.">dnnl::graph::logical_tensor</a> for first <code>Convolution</code> op. </p><div class="fragment"><div class="line">    logical_tensor conv0_src_desc {0, data_type::f32,</div>
<div class="line">            conv0_input_dims, layout_type::undef};</div>
<div class="line">    logical_tensor conv0_weight_desc {1, data_type::f32,</div>
<div class="line">            conv0_weight_dims, layout_type::undef};</div>
<div class="line">    logical_tensor conv0_dst_desc {2, data_type::f32, 4,</div>
<div class="line">            layout_type::undef};</div>
</div><!-- fragment --><p>Create first <code>Convolution</code> op (<a class="el" href="classdnnl_1_1graph_1_1op.html" title="An op object.">dnnl::graph::op</a>) and attaches attributes to it, such as <code>strides</code>, <code>pads_begin</code>, <code>pads_end</code>, <code>data_format</code>, etc. </p><div class="fragment"><div class="line">    op conv0(3, op::kind::Convolution, {conv0_src_desc, conv0_weight_desc},</div>
<div class="line">            {conv0_dst_desc}, <span class="stringliteral">&quot;conv0&quot;</span>);</div>
<div class="line">    conv0.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(op::attr::strides, {4, 4});</div>
<div class="line">    conv0.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(op::attr::pads_begin, {0, 0});</div>
<div class="line">    conv0.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(op::attr::pads_end, {0, 0});</div>
<div class="line">    conv0.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(op::attr::dilations, {1, 1});</div>
<div class="line">    conv0.set_attr&lt;int64_t&gt;(op::attr::groups, 1);</div>
<div class="line">    conv0.set_attr&lt;std::string&gt;(op::attr::data_format, <span class="stringliteral">&quot;NCX&quot;</span>);</div>
<div class="line">    conv0.set_attr&lt;std::string&gt;(op::attr::filter_format, <span class="stringliteral">&quot;OIX&quot;</span>);</div>
</div><!-- fragment --><p>Create input/output logical tensors for first <code>BiasAdd</code> op. </p><div class="fragment"><div class="line">    logical_tensor conv0_bias_desc {4, data_type::f32,</div>
<div class="line">            conv0_bias_dims, layout_type::undef};</div>
<div class="line">    logical_tensor conv0_bias_add_dst_desc {5, data_type::f32,</div>
<div class="line">            4, layout_type::undef};</div>
</div><!-- fragment --><p>Create first <code>BiasAdd</code> op. </p><div class="fragment"><div class="line">    op conv0_bias_add(6, op::kind::BiasAdd, {conv0_dst_desc, conv0_bias_desc},</div>
<div class="line">            {conv0_bias_add_dst_desc}, <span class="stringliteral">&quot;conv0_bias_add&quot;</span>);</div>
<div class="line">    conv0_bias_add.set_attr&lt;std::string&gt;(op::attr::data_format, <span class="stringliteral">&quot;NCX&quot;</span>);</div>
</div><!-- fragment --><p>Create output logical tensors for first <code>Relu</code> op. </p><div class="fragment"><div class="line">    logical_tensor relu0_dst_desc {7, data_type::f32, 4,</div>
<div class="line">            layout_type::undef};</div>
</div><!-- fragment --><p>Create first <code>Relu</code> op. </p><div class="fragment"><div class="line">    op relu0(8, op::kind::ReLU, {conv0_bias_add_dst_desc}, {relu0_dst_desc},</div>
<div class="line">            <span class="stringliteral">&quot;relu0&quot;</span>);</div>
</div><!-- fragment --><p>Create input/output logical tensors for second <code>Convolution</code> op. </p><div class="fragment"><div class="line">    logical_tensor conv1_weight_desc {9, data_type::f32,</div>
<div class="line">            conv1_weight_dims, layout_type::undef};</div>
<div class="line">    logical_tensor conv1_dst_desc {10, data_type::f32, 4,</div>
<div class="line">            layout_type::undef};</div>
</div><!-- fragment --><p>Create second <code>Convolution</code> op and also attaches required attributes to it. </p><div class="fragment"><div class="line">    op conv1(11, op::kind::Convolution, {relu0_dst_desc, conv1_weight_desc},</div>
<div class="line">            {conv1_dst_desc}, <span class="stringliteral">&quot;conv1&quot;</span>);</div>
<div class="line">    conv1.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(op::attr::strides, {1, 1});</div>
<div class="line">    conv1.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(op::attr::pads_begin, {0, 0});</div>
<div class="line">    conv1.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(op::attr::pads_end, {0, 0});</div>
<div class="line">    conv1.set_attr&lt;std::vector&lt;int64_t&gt;&gt;(op::attr::dilations, {1, 1});</div>
<div class="line">    conv1.set_attr&lt;int64_t&gt;(op::attr::groups, 1);</div>
<div class="line">    conv1.set_attr&lt;std::string&gt;(op::attr::data_format, <span class="stringliteral">&quot;NCX&quot;</span>);</div>
<div class="line">    conv1.set_attr&lt;std::string&gt;(op::attr::filter_format, <span class="stringliteral">&quot;OIX&quot;</span>);</div>
</div><!-- fragment --><p>Create input/output logical tensors for second <code>BiasAdd</code> op. </p><div class="fragment"><div class="line">    logical_tensor conv1_bias_desc {12, data_type::f32,</div>
<div class="line">            conv1_bias_dims, layout_type::undef};</div>
<div class="line">    logical_tensor conv1_bias_add_dst_desc {13, data_type::f32,</div>
<div class="line">            4, layout_type::undef};</div>
</div><!-- fragment --><p>Create second <code>BiasAdd</code> op. </p><div class="fragment"><div class="line">    op conv1_bias_add(14, op::kind::BiasAdd, {conv1_dst_desc, conv1_bias_desc},</div>
<div class="line">            {conv1_bias_add_dst_desc}, <span class="stringliteral">&quot;conv1_bias_add&quot;</span>);</div>
<div class="line">    conv1_bias_add.set_attr&lt;std::string&gt;(op::attr::data_format, <span class="stringliteral">&quot;NCX&quot;</span>);</div>
</div><!-- fragment --><p>Create output logical tensors for second <code>Relu</code> op. </p><div class="fragment"><div class="line">    logical_tensor relu1_dst_desc {15, data_type::f32, 4,</div>
<div class="line">            layout_type::undef};</div>
</div><!-- fragment --><p>Create second <code>Relu</code> op. </p><div class="fragment"><div class="line">    op relu1(16, op::kind::ReLU, {conv1_bias_add_dst_desc}, {relu1_dst_desc},</div>
<div class="line">            <span class="stringliteral">&quot;relu1&quot;</span>);</div>
</div><!-- fragment --><p>Finally, those created ops will be added into the graph. The graph internally will maintain a list to store all of these ops.</p>
<dl class="section note"><dt>Note</dt><dd>The order of adding op doesn't matter.</dd></dl>
<div class="fragment"><div class="line">    g.add_op(conv0);</div>
<div class="line">    g.add_op(conv0_bias_add);</div>
<div class="line">    g.add_op(relu0);</div>
<div class="line">    g.add_op(conv1);</div>
<div class="line">    g.add_op(conv1_bias_add);</div>
<div class="line">    g.add_op(relu1);</div>
</div><!-- fragment --><p>After finished above operations, we can get partitions by calling <a class="el" href="classdnnl_1_1graph_1_1graph.html#a116d3552e3b0e6c739a1564329bde014" title="Gets filtered partitions from a graph.">dnnl::graph::graph::get_partitions()</a>. Here we can also specify the <a class="el" href="classdnnl_1_1graph_1_1partition.html#a439c0490ea8ea85f2a12ec7b320a9a3c" title="Policy specifications for partitioning.">dnnl::graph::partition::policy</a> to get different partitions.</p>
<p>In this example, the graph will be partitioned into two partitions:</p><ol type="1">
<li>conv0 + conv0_bias_add + relu0</li>
<li>conv1 + conv1_bias_add + relu1</li>
</ol>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> partitions = g.get_partitions();</div>
</div><!-- fragment --><p>Contains the ids of logical tensors which will be set with any layout</p>
<p>This is a helper function which helps decide which logical tensor is needed to be set with <code><a class="el" href="classdnnl_1_1graph_1_1logical__tensor.html#ad3fcaff44671577e56adb03b770f4867a100b8cad7cf2a56f6df78f171f97a1ec" title="Any means to let the library to decide the layout for a tensor during partition compilation.">dnnl::graph::logical_tensor::layout_type::any</a></code> layout. Typically, users need implement the similar logic in their code for best performance.</p>
<p>Below codes are to create runtime objects like allocator, engine and stream. Unlike CPU example, users need provide sycl device, sycl context, and sycl queue. oneDNN Graph provides different interoperability APIs which are defined at <code><a class="el" href="dnnl__graph__sycl_8hpp_source.html">dnnl_graph_sycl.hpp</a></code>.</p>
<h2><a class="anchor" id="sycl_get_started_cpp_compile"></a>
Compile partition</h2>
<p>In the real case, users like framework should provide device information at this stage. But in this example, we just use a self-defined device to simulate the real behavior.</p>
<p>Create a <a class="el" href="classdnnl_1_1graph_1_1allocator.html" title="An allocator.">dnnl::graph::allocator</a> with two user-defined <a class="el" href="group__dnnl__graph__api__allocator.html#ga74d9aec0f8f9c3a9da2cbf2df5cc1e8c" title="Allocation call-back function interface for SYCL device.">dnnl_graph_sycl_allocate_f</a> and <a class="el" href="group__dnnl__graph__api__allocator.html#ga77936c59bb8456176973fa03f990298f" title="brief Deallocation call-back function interface for SYCL device">dnnl_graph_sycl_deallocate_f</a> call-back functions. </p><div class="fragment"><div class="line">    allocator alloc = sycl_interop::make_allocator(sycl_malloc_wrapper, sycl_free_wrapper);</div>
</div><!-- fragment --><p> Define SYCL queue (code outside of oneDNN graph) </p><div class="fragment"><div class="line">    sycl::queue q = (ekind == engine::kind::gpu)</div>
<div class="line">            ? sycl::queue(gpu_selector {}, sycl::property::queue::in_order {})</div>
<div class="line">            : sycl::queue(cpu_selector {}, sycl::property::queue::in_order {});</div>
</div><!-- fragment --><p>Create a <a class="el" href="classdnnl_1_1graph_1_1engine.html" title="An execution engine.">dnnl::graph::engine</a> based on SYCL device and context. Also, set a user-defined <a class="el" href="classdnnl_1_1graph_1_1allocator.html" title="An allocator.">dnnl::graph::allocator</a> to this engine.</p>
<div class="fragment"><div class="line">    engine eng = sycl_interop::make_engine(q.get_device(), q.get_context(), alloc);</div>
</div><!-- fragment --><p>Create a <a class="el" href="classdnnl_1_1graph_1_1stream.html" title="An execution stream.">dnnl::graph::stream</a> on the given engine</p>
<div class="fragment"><div class="line">    <a class="code" href="classdnnl_1_1graph_1_1stream.html">dnnl::graph::stream</a> strm = sycl_interop::make_stream(eng, q);</div>
<div class="ttc" id="aclassdnnl_1_1graph_1_1stream_html"><div class="ttname"><a href="classdnnl_1_1graph_1_1stream.html">dnnl::graph::stream</a></div><div class="ttdoc">An execution stream.</div><div class="ttdef"><b>Definition:</b> dnnl_graph.hpp:318</div></div>
</div><!-- fragment --><p>Compile the partition 0 to generate compiled partition with the input and output logical tensors. </p><div class="fragment"><div class="line">            compiled_partition cp = partition.compile(inputs, outputs, eng);</div>
</div><!-- fragment --><p>Execute the compiled partition on the specified stream. </p><div class="fragment"><div class="line">            cp.execute(strm, inputs_ts, outputs_ts);</div>
</div><!-- fragment --></div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="footer">
    <script>
        $('#top').prependTo($('#side-nav'));
    </script>
    <div class="footer-wrapper">
        <hr>
        <ul class="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>